{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:45.792905Z",
     "start_time": "2025-06-17T17:33:41.678450Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\virt\\dla_reforgiato\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM ,BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e7ace69f5260c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:45.799149Z",
     "start_time": "2025-06-17T17:33:45.796157Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "DATA_PATH = \"data/spam_or_not_spam.csv\"\n",
    "MAX_LENGTH = 512\n",
    "hf_token = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc13f04d0b9a756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:46.910191Z",
     "start_time": "2025-06-17T17:33:46.039001Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True,token = hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0725f16641754c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:52.714122Z",
     "start_time": "2025-06-17T17:33:46.926445Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34394e5618cfd0d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:52.734932Z",
     "start_time": "2025-06-17T17:33:52.731812Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_chat(example):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'Classify this email as spam (1) or not spam (0): \"{example[\"email\"]}\"'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": str(example[\"label\"])\n",
    "        }\n",
    "    ]\n",
    "    full_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": full_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ccd6bc0a8ee7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:52.754700Z",
     "start_time": "2025-06-17T17:33:52.751415Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"email\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f2eb18c56f6faa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:52.829901Z",
     "start_time": "2025-06-17T17:33:52.772056Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/spam_or_not_spam.csv\").dropna(subset=[\"email\", \"label\"])\n",
    "df = df.dropna(subset=['email', 'label'])\n",
    "\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=1)\n",
    "\n",
    "train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c0d2200cca1081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:33:53.828212Z",
     "start_time": "2025-06-17T17:33:52.853280Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 1323.69 examples/s]\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 1466.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val = val.map(tokenize)\n",
    "test = test.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4ca3db7c5d48bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:34:20.591601Z",
     "start_time": "2025-06-17T17:33:53.850758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Classification Report on Validation Set ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8330    0.9975    0.9078       400\n",
      "           1     0.0000    0.0000    0.0000        80\n",
      "\n",
      "    accuracy                         0.8313       480\n",
      "   macro avg     0.4165    0.4988    0.4539       480\n",
      "weighted avg     0.6942    0.8313    0.7565       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions, references = [], []\n",
    "\n",
    "for example in val:\n",
    "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(\"cuda\")\n",
    "    attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=2,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    pred = 1 if \"1\" in decoded[-3:] else 0\n",
    "    true = int(example[\"label\"])\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(true)\n",
    "\n",
    "print(\"== Classification Report on Validation Set ==\")\n",
    "print(classification_report(references, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770d28ee7609d40d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T17:34:53.984619Z",
     "start_time": "2025-06-17T17:34:20.635246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Classification Report on Test Set ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8208    0.9959    0.8999       492\n",
      "           1     0.3333    0.0093    0.0180       108\n",
      "\n",
      "    accuracy                         0.8183       600\n",
      "   macro avg     0.5771    0.5026    0.4590       600\n",
      "weighted avg     0.7330    0.8183    0.7412       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions, references = [], []\n",
    "\n",
    "for example in test:\n",
    "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(\"cuda\")\n",
    "    attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=2,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "    pred = 1 if \"1\" in decoded[-3:] else 0\n",
    "    true = int(example[\"label\"])\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(true)\n",
    "\n",
    "print(\"== Classification Report on Test Set ==\")\n",
    "print(classification_report(references, predictions, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
